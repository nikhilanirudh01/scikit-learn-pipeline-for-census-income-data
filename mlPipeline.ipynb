{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikhilanirudh01/scikit-learn-pipeline-for-census-income-data/blob/main/mlPipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Name: NIKHIL ANIRUDH NAISHADHAM\n",
        "#ASU Id: 1229454124\n",
        "#Date: November 14th, 2024"
      ],
      "metadata": {
        "id": "HmhSfJIWmKNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PART 1**: Baseline ML Pipeline to create an estimator with cross validation."
      ],
      "metadata": {
        "id": "FwL0YbP0niK7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWduD7HClw1A",
        "outputId": "77af0330-886c-4843-b1b1-440f5f4fdb0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Average Classification Score is:  0.7933726904535288\n"
          ]
        }
      ],
      "source": [
        "#import the datasets:\n",
        "from sklearn.datasets import load_svmlight_file as load_SVM\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.pipeline import make_pipeline as MPL\n",
        "from sklearn.model_selection import cross_val_score as CVS\n",
        "from scipy.sparse import csr_matrix as CSR\n",
        "import numpy as npy\n",
        "\n",
        "#Import the dataset into code:\n",
        "(x_train, y_train) = load_SVM('/content/a9a.txt')\n",
        "\n",
        "#converting sparse data into dense data:\n",
        "xTrain_dense = x_train.todense()\n",
        "xTrain_dense = npy.asarray(xTrain_dense)\n",
        "\n",
        "#creating a standrd scalar object\n",
        "SS = StandardScaler ()\n",
        "\n",
        "#Create decision tree object:\n",
        "DT = DecisionTreeClassifier()\n",
        "\n",
        "#Create a pipeline:\n",
        "na_pipe = MPL(SS, DT)\n",
        "\n",
        "#Using cross validation score:\n",
        "crossVal_Score = CVS(na_pipe, xTrain_dense, y_train, cv = 5)\n",
        "\n",
        "print(\"The Average Classification Score is: \", crossVal_Score.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PART 2**: Hyperparameter Tuning with GridSearchCV"
      ],
      "metadata": {
        "id": "Sj6ONr_RnndX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the datasets:\n",
        "from sklearn.datasets import load_svmlight_file as load_SVM\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.pipeline import make_pipeline as MPL\n",
        "from sklearn.model_selection import cross_val_score as CVS\n",
        "from scipy.sparse import csr_matrix as CSR\n",
        "from sklearn.model_selection import GridSearchCV as GSCV\n",
        "import numpy as npy\n",
        "\n",
        "#Import the dataset into code:\n",
        "(x_train, y_train) = load_SVM('/content/a9a.txt')\n",
        "\n",
        "#converting sparse data into dense data:\n",
        "xTrain_dense = x_train.todense()\n",
        "xTrain_dense = npy.asarray(xTrain_dense)\n",
        "\n",
        "#creating a standrd scalar object\n",
        "SS = StandardScaler ()\n",
        "\n",
        "#Create decision tree object:\n",
        "DT = DecisionTreeClassifier()\n",
        "\n",
        "#Create a pipeline:\n",
        "na_pipe = MPL(SS, DT)\n",
        "\n",
        "#Create parameters:\n",
        "gridParameters = {\n",
        "    'decisiontreeclassifier__criterion': ['gini', 'entropy'],\n",
        "    'decisiontreeclassifier__max_depth': [5, 10, 15, 20]\n",
        "}\n",
        "\n",
        "#Create grid search:\n",
        "gridSearch = GSCV(na_pipe, gridParameters, cv = 5)\n",
        "gridSearch.fit(xTrain_dense, y_train)\n",
        "\n",
        "print(\"Best Parameter is: \", gridSearch.best_params_)\n",
        "print(\"Best Score is: \", gridSearch.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_E-ZCLKEnnEL",
        "outputId": "933d0952-93a0-469e-a739-cab2a5f550ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameter is:  {'decisiontreeclassifier__criterion': 'entropy', 'decisiontreeclassifier__max_depth': 10}\n",
            "Best Score is:  0.8351710536590776\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PART 3**: Train-Test Split with Tuned Model Evaluation"
      ],
      "metadata": {
        "id": "tevhMnE8sCLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the datasets:\n",
        "from sklearn.datasets import load_svmlight_file as load_SVM\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.pipeline import make_pipeline as MPL\n",
        "from sklearn.model_selection import cross_val_score as CVS\n",
        "from sklearn.model_selection import train_test_split as TTSPlit\n",
        "from sklearn.model_selection import GridSearchCV as GSCV\n",
        "import numpy as npy\n",
        "\n",
        "#Import the dataset into code:\n",
        "(x, y) = load_SVM('/content/a9a.txt')\n",
        "\n",
        "#converting sparse data into dense data:\n",
        "x = x.todense()\n",
        "x = npy.asarray(x)\n",
        "\n",
        "#converting sparse data into dense data:\n",
        "(x_train, x_test, y_train, y_test) = TTSPlit(x, y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "#creating a standrd scalar object\n",
        "SS = StandardScaler ()\n",
        "\n",
        "#Create decision tree object:\n",
        "DT = DecisionTreeClassifier()\n",
        "\n",
        "#Create a pipeline:\n",
        "na_pipe = MPL(SS, DT)\n",
        "\n",
        "#Create parameters:\n",
        "gridParameters = {\n",
        "    'decisiontreeclassifier__criterion': ['gini', 'entropy'],\n",
        "    'decisiontreeclassifier__max_depth': [5, 10, 15, 20]\n",
        "}\n",
        "\n",
        "#Create grid search:\n",
        "gridSearch = GSCV(na_pipe, gridParameters, cv = 5)\n",
        "gridSearch.fit(x_train, y_train)\n",
        "\n",
        "#fit SS on training data\n",
        "SS.fit(x_train)\n",
        "\n",
        "#get the best model:\n",
        "bestMod = gridSearch.best_estimator_\n",
        "\n",
        "#scale the test data\n",
        "x_testScale = SS.transform(x_test)\n",
        "\n",
        "#obtain accuracy\n",
        "tesAcc = bestMod.score(x_testScale, y_test)\n",
        "\n",
        "print(\"Test Accuracy is: \", tesAcc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEnvyONWnqPO",
        "outputId": "ff43fbbe-2d63-4bab-ec16-cb11fc082ce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy is:  0.7660064486411792\n"
          ]
        }
      ]
    }
  ]
}